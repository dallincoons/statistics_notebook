---
title: "Testing benchmarks"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
---

## Overview

As a software developer I write a lot of tests to assert that my code does what it's supposed to do. Sometimes that involves sending HTTP requests, which is essentially what happens when you go to a website in a browser. The idea is to create a test that will send a request and check the correct response is sent back. 

The problem is that the process of sending HTTP requests tends to be slow and ultimately slows down the test suite. I wanted to find out exactly how costly each HTTP request is to overall speed of the test.

On my laptop, firing one HTTP request in a test takes about .78 seconds. My null hypothesis is that two HTTP requests will take .78 * 2 = 1.56, three requests will be .78 * 3 = 2.34 and so on.


$$
\underbrace{{Y_i}}_\text{Time} = \beta_0 + \beta_1\underbrace{{Xi}}_\text{Number of requests} + \epsilon_i \quad \text{where} \ \epsilon_i \ \sim N(0, \sigma^2)
$$

<hr>

$$
  H_0 : \beta_1 = .78
$$

$$
  H_a : \beta_1 \not= .78
$$

<hr>

$$
  \alpha = .05
$$

```{r, include=FALSE}
library(mosaic)
library(pander)
library(tidyverse)
library(DT) 
library(ggplot2)
```


```{r}
benchmarks <- read.csv("../Data/benchmarks.csv", header=TRUE)

benchmark.lm <- lm(time ~ num_calls, data=benchmarks)

ggplot(benchmarks, aes(x = num_calls, y = time)) +
  geom_point() +
  geom_abline(aes(colour="Regression", intercept = benchmark.lm$coefficients[1], slope = benchmark.lm$coefficients[2])) +
  scale_x_continuous(breaks = seq(1, 10, by = 1)) +
  scale_y_continuous(breaks = seq(1, 6, by = 1)) +
  labs(x = "Number of HTTP requests", y = "Time (seconds)") +
  geom_abline(aes(intercept = benchmark.lm$coefficients[1], slope = .78, color = "Null Hypoethesis")) +
  theme_bw() +
  labs(colour="") +
  scale_colour_manual(values=c("gray", "skyblue"))
```

## Technical details

The slope of the regression line is .59 vs the expected .78, wich would indicate that for every HTTP request that is added, .59 seconds are added to the time. This is .19 seconds less than what would be expected if the null hypothesis were true.

The residual standard error, .06142, shows the average variation from the mean is small, if the chart wasn't convincing enough. The $R^2$ value of .9987 proves us that the vast majority (more than 99%) of variation can be explained by the regression line.

Since the data has such low variance from the regression line it's not suprising to see that the standard error that is rather small at .003024. This gives us a 95% confidence interval of 0.576368 to 0.588529. We can be 95% condident that the true mean of lies between those points, or 95% percent of the time this experiment is performed the regression slope will lie between those values.

The regression line from this experiment is 65 standard errors away from the null hypotheis, which is what the t value represents. Therefore it's very unlikely that the null hypothesis would happen by chance, which the p-value of 1.484e-48 confirms, and because it is below the threshold of believability (.05) we can safely reject the null hypothesis.


```{r}
benchmark.lm <- lm(time ~ num_calls, data=benchmarks, offset = benchmarks$num_calls * .78)
pander(summary(benchmark.lm))
```

## Interpretation

It would be interesting to learn the technical details of why sending multiple HTTP requests in the same test function doesn't result in a linear time increase. I suspect somewhere along the way, something is being cached so that subsequent requests don't take as long. This means that adding multiple HTTP requests to a test function in the name of speeding up tests may have some merit.

## Diagnostics

The most important consideration, Residual Vs Fitted shows what could be some non-linearity in the data, though it doesn't have a smooth, clear pattern it's probably not something will invalidate the results. Constant variance isn't terrible but it looks like there's at least a couple outliers which may be negatively affecting it. It's right on the edges
The Q-Q plot shows some issues with normality, and Residuals Vs Order looks fairly patternless with maybe a hint of a trend.

```{r, fig.height=4}
par(mfrow=c(1,3))
plot(benchmark.lm, which=1:2)
plot(benchmark.lm$residuals, main="Residuals vs Order", xlab="", ylab="Residuals")
```

