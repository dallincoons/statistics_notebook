---
title: "Midterm Decision"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
---

```{r, include = FALSE}
library(tidyverse)
library(ggplot2)
library(car)
library(pander)
```


```{r, warning=FALSE}
raw_grades <- read.csv('../Data/Math425Grades.csv')
```


To get an initial understanding of the data, I plotted the points with Final Exam as the explanatory variable and Midterm as the response. I  drew a simple regression line to visualize what we're dealing with.

```{r, warning=FALSE}
simple.lm <- lm(FinalExam ~ Midterm, data = raw_grades)

ggplot(raw_grades, aes(x = Midterm, y = FinalExam)) +
  geom_point() +
  geom_abline(intercept = simple.lm$coefficients[1], slope = simple.lm$coefficients[2]) +
  theme_bw()
```

The simple regression gives us an r-squared value of `r summary(simple.lm)$r.squared`. Let's see if we can do better.

One thing that sticks out immediately are the outliers. There are a few scores at zero or near zero. I have to assume these were people who didn't take the test or didn't try or quit the test early. Neither one of those situations are relevant because I plan on taking the test and actually giving it some effort, and so I removed those from the data. However I decided to keep the point at 0 at the bottom left because it seems to strengthen the regression.

```{r}
grades <- raw_grades %>%
  slice(-c(1,5, 24))

simple.lm <- lm(FinalExam ~ Midterm, data = grades)

b <- coef(simple.lm)

ggplot(grades, aes(x = Midterm, y = FinalExam)) +
  geom_point() +
  stat_function(fun = function(x) b[1] + b[2]*x, color="skyblue") +
  theme_bw()
```

The new simple regression gives us an r-squared value of `r summary(simple.lm)$r.squared`, which is already much improved.

I'm going to try a few things to try and determine if there's a better model. First I'm going to add a Loess curve:

```{r}
ggplot(grades, aes(x = Midterm, y = FinalExam)) +
  geom_point() +
  geom_smooth(method="loess", span=0.8,
               se=FALSE, 
               method.args=list(degree=2)) +
  theme_bw()

quadratic.lm <- lm(FinalExam ~ Midterm + I(Midterm^2), data = grades)
cubic.lm <- lm(FinalExam ~ Midterm + I(Midterm^2) + I(Midterm^3), data = grades)
```

There's a little bit of a curve, possibly suggesting a quadratic or cubic model. The quadric model yields an r-squared of `r summary(quadratic.lm)$r.squared` and the cubic model `r summary(cubic.lm)$r.squared`, neither of which are enough of an improvement to justify the loss of interpretability that would result in using them.

Let's check if there's a transformation on the simple linear model that would be helpful.

```{r}
boxCox(lm(FinalExam+1 ~ Midterm, data = grades))
```

The confidence interval contains 1, indicating a transformation wouldn't be useful.

The full output of the regression currently:

```{r}
summary(simple.lm)


```

We could make a prediction using this model, but it's also good to keep in mind that predictions aren't just a precise point, it's a range of values. For the slope coefficient, the standard error (the estimate of the standard deviation) is .11. We can be 95% confident that the true slope is within 2 standard errors (1.96 really) of the estimated mean, which would be about .22 plus or minus the slope estimate of .93. That seems like a large range. Indeed if we look at the prediction, 81.82 is the predicted value, but the prediction interval goes from 56.2 to 107.46 (effectively 100).

```{r}
predict(simple.lm, newdata = data.frame(Midterm = 84), interval = "prediction")
```

I don't want to base my decision from that kind of prediction interval ao if possible I'd like to try and narrow it down.

Something that bothers me about this regression is that there appears to be two distinct groups. There's a group on the lower right hand side, with final exam scores of around 50, and then there's a group of students above that around the 75-80 range. 

These groups can also be seens using a residuals vs fitted plot

```{r}
plot(simple.lm, which = 1)
```

I'll pull up a pairs plot using the resdiuals and fitted values from the regression I ran earlier to see if there are any additional variables that can provide insight.

```{r}
pairs(cbind(R = simple.lm$residuals, Fit = simple.lm$fit, grades))
```

It doesn't look like there's much more information we can squeeze out of the Midterm variable, and I'm not seeing any clear pattens in SkillsQuizzesTotal or Gender.

At first I wasn't sure if there was anywhere to go from here but I got the idea from another student in class that perhaps what explains the two groups is how well the students performed _relative to_ the midterm score. 

I decided to add a new synthetic column to the dataset that accounts for differences in score by subtracting the Midterm score from the FinalExam score, and setting the value to 1 if they had a lower score, and a 0 if they scored equal to or higher than the Midterm.

```{r}
grades_difference <- grades %>% mutate(ScoredLower = ifelse(FinalExam - Midterm < 0, 1, 0))
```

Lets look at the pairs plot with this new column.

```{r}
pairs(grades_difference, panel = panel.smooth)
```

It looks like maybe there's something there. Lets try adding that variable to the regression.

```{r}
summary(lm(FinalExam ~ Midterm + ScoredLower + Midterm:ScoredLower, data = grades_difference))
```

It looks like we're onto something! Let's get rid of the insignifant coefficient, to see if that helps.

```{r}
two.lines <- lm(FinalExam ~ Midterm + Midterm:ScoredLower, data = grades_difference)
b <- coef(two.lines)
summary(two.lines)
```

That standard error looks much better, and now we should get tighter prediction intervals:

```{r}
predict(two.lines, newdata = data.frame(Midterm = 84, ScoredLower = 0), interval = "prediction")
predict(two.lines, newdata = data.frame(Midterm = 84, ScoredLower = 1), interval = "prediction")
```

Bam! I love statistics.

Let's see what the plot looks like.

```{r}
prediction_upper <- predict(two.lines, newdata = data.frame(Midterm = 84, ScoredLower = 0), interval = "prediction")
prediction_lower <- predict(two.lines, newdata = data.frame(Midterm = 84, ScoredLower = 1), interval = "prediction")

ggplot(grades, aes(x = Midterm, y = FinalExam)) +
  geom_point() +
  stat_function(fun = function(x) b[1] + b[2]*x, color="skyblue") +
  stat_function(fun = function(x) b[1] + (b[2] + b[3])*x, color="firebrick") +
  geom_rect(data = grades_difference, xmin = 83, xmax = 85, ymin = prediction_upper[2], ymax = prediction_upper[3], alpha = 0.03, fill = "skyblue") + 
  geom_point(x = 84, y = prediction_upper[1], size = 2, color = "#eeeeee") +
  geom_rect(data = grades_difference, xmin = 83, xmax = 85, ymin = prediction_lower[2], ymax = prediction_lower[3], alpha = 0.03, fill = "firebrick") + 
  geom_point(x = 84, y = prediction_lower[1], size = 2, color = "#eeeeee") +
  theme_bw()
```

and let's check linearity and constant variance

```{r}
plot(two.lines, which = 1)
```

It looks like there's constant variance as well as decent enough linearity.

I feel good enough about this that I'll formally define the model:

$$
\underbrace{Y_i}_{\text{Final exam score}} = \underbrace{\beta_0}_{\text{y-intercept baseline}} + 
\underbrace{\beta_1}_{\text{slope baseline}}\underbrace{X_{1i}}_\text{Midterm score} +
\underbrace{\beta_2}_\text{change in slope}
\underbrace{X1iX2i}_\text{Midterm:LowerScore interaction} +
\epsilon_i
$$

## Decision

Now I have to decide which regression line I'm going to use to predict my score. I could look at midterm exams from the past and compare them to how well I did on the final exams. It would be ideal to look at Math 325 because I would assume the caliber of the test given would be similar, but apparently there was no midterm in that class. It seems like historically my exams stay fairly on par from test to test, so I'll optimistically assume I'm in the upper group. 

The formula for calculating the exams part of our grade is given in I-learn as
$$
  \text{Exam Score} = \text{0.7(Final Exam Score)} + \text{0.3(Midterm Score)}
$$

Assuming my Final Exam score is 89.8, my exam score will be 

$$
  \text{.7(89.8) + .3(84) = 88}
$$
88 is in in line will my predicted final exam score, so I will opt to <b>keep my midterm exam</b>.
